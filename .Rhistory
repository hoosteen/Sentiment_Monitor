BB$day, sep = "-"),
format = "%Y-%m-%d")
BB.text$Date <- strptime(as.character(BB.text$Date),
"%Y-%m-%d")
# Reassign column names
colnames(BB.text) <- c("text", "year", "date")
## ///////CLEANING DATA WITH TM_MAP/////// ##
# tm_map allows transformation to a corpus
bb_corpus <- Corpus(VectorSource(BB.text))
# stem the documents
bb.text_stm <- tm_map(bb_corpus, stemDocument)
# Standard stopwords ie the SMART list are in TM
stnd.stopwords<- stopwords("SMART")
## ////// TRANSFORMING DATA FOR SORTING ////// ##
# useful to eliminate words that lack
# discriminatory power. bb.tf will be used as a control
# for the creation of our term-document matrix.
bb.tf <- list(weighting = weightTf,
stopwords = bb.stopwords,
removePunctuation = TRUE,
tolower = TRUE,
minWordLength = 4,
removeNumbers = TRUE)
# create a term-document matrix
bb_tdm <- TermDocumentMatrix(bb_corpus,
control = bb.tf)
# sorting frequent words in TDM to ID words lacking
# discriminatory power to add to custom stopword
# lexicon
bb.frequent <- sort(rowSums(as.matrix(bb_tdm)),
decreasing = TRUE)
# further exploration
bb.frequent[1:30]
# look at terms with a minimum frequency
findFreqTerms(bb_tdm, lowfreq = 60)
# positive words added to lexicon:
pos.words<- c(pos_all, "spend", "buy", "earn", "hike", "increase",
"increases", "development", "expansion", "raise", "surge", "add",
"added", "advanced", "advances", "boom", "boosted", "boosting",
"waxed", "upbeat", "surge")
#Adding to negative words
neg.words = c(neg_all, "earn", "shortfall", "weak", "fell",
"decreases", "decreases", "decreased", "contraction", "cutback",
"cuts", "drop", "shrinkage", "reduction", "abated", "cautious",
"caution", "damped", "waned", "undermine", "unfavorable", "soft",
"softening", "soften", "softer", "sluggish", "slowed", "slowdown",
"slower", "recession")
# Corpus exploration indicates these words should
# be removed from our corpus
bb.stopwords <- c(stnd.stopwords, "district",
"districts", "reported", "noted",
"city", "cited", "activity",
"contacts", "chicago", "dallas",
"kansas", "san", "richmond",
"francisco", "cleveland", "atlanta",
"sales", "boston", "york",
"philadelphia", "minneapolis",
"louis", "services", "year",
"levels")
## ////// TRANSFORMING DATA FOR SORTING ////// ##
# create a term-document matrix
bb_tdm <- TermDocumentMatrix(bb_corpus,
control = bb.tf)
# sorting frequent words in TDM to ID words lacking
# discriminatory power to add to custom stopword
# lexicon
bb.frequent <- sort(rowSums(as.matrix(bb_tdm)),
decreasing = TRUE)
# further exploration
bb.frequent[1:30]
# look at terms with a minimum frequency
findFreqTerms(bb_tdm, lowfreq = 60)
# positive words added to lexicon:
pos.words<- c(pos_all, "spend", "buy", "earn", "hike", "increase",
"increases", "development", "expansion", "raise", "surge", "add",
"added", "advanced", "advances", "boom", "boosted", "boosting",
"waxed", "upbeat", "surge")
#Adding to negative words
neg.words = c(neg_all, "earn", "shortfall", "weak", "fell",
"decreases", "decreases", "decreased", "contraction", "cutback",
"cuts", "drop", "shrinkage", "reduction", "abated", "cautious",
"caution", "damped", "waned", "undermine", "unfavorable", "soft",
"softening", "soften", "softer", "sluggish", "slowed", "slowdown",
"slower", "recession")
# Corpus exploration indicates these words should
# be removed from our corpus
bb.stopwords <- c(stnd.stopwords, "district",
"districts", "reported", "noted",
"city", "cited", "activity",
"contacts", "chicago", "dallas",
"kansas", "san", "richmond",
"francisco", "cleveland", "atlanta",
"sales", "boston", "york",
"philadelphia", "minneapolis",
"louis", "services", "year",
"levels")
# useful to eliminate words that lack
# discriminatory power. bb.tf will be used as a control
# for the creation of our term-document matrix.
bb.tf <- list(weighting = weightTf,
stopwords = bb.stopwords,
removePunctuation = TRUE,
tolower = TRUE,
minWordLength = 4,
removeNumbers = TRUE)
# create a new object "bad" that will hold missing data
bad <- is.na(BB)
# return all missing elements
BB[bad]
## ///////CLEANING DATA WITH REGEX/////// ##
# gsub substitutes punctuation with a space
BB$text <- gsub('[[:punct:]]', ' ', BB$text)
# gsub substitues character classes that do not
# give an output, such as feed, bckspce & tab w/space
BB$text <- gsub('[[:cntrl:]]', ' ', BB$text)
# gsub subs numerical values with digits >=1 w/' '
BB$text <- gsub('//d+', ' ', BB$text)
## ///////CLEANING DATA WITH SHUFFLING/////// ##
# simplify the data frame by keeping the cleaned text,
# year and concatenated version of year/month/day
BB.text <- as.data.frame(BB$text)
BB.text$year <- BB$year
BB.text$Date <- as.Date(paste(BB$year,
BB$month,
BB$day, sep = "-"),
format = "%Y-%m-%d")
BB.text$Date <- strptime(as.character(BB.text$Date),
"%Y-%m-%d")
# Reassign column names
colnames(BB.text) <- c("text", "year", "date")
## ///////CLEANING DATA WITH TM_MAP/////// ##
# tm_map allows transformation to a corpus
bb_corpus <- Corpus(VectorSource(BB.text))
# stem the documents
bb.text_stm <- tm_map(bb_corpus, stemDocument)
# Standard stopwords ie the SMART list are in TM
stnd.stopwords<- stopwords("SMART")
## ////// TRANSFORMING DATA FOR SORTING ////// ##
# create a term-document matrix
bb_tdm <- TermDocumentMatrix(bb_corpus,
control = bb.tf)
# sorting frequent words in TDM to ID words lacking
# discriminatory power to add to custom stopword
# lexicon
bb.frequent <- sort(rowSums(as.matrix(bb_tdm)),
decreasing = TRUE)
# further exploration
bb.frequent[1:30]
# look at terms with a minimum frequency
findFreqTerms(bb_tdm, lowfreq = 60)
# positive words added to lexicon:
pos.words<- c(pos_all, "spend", "buy", "earn", "hike", "increase",
"increases", "development", "expansion", "raise", "surge", "add",
"added", "advanced", "advances", "boom", "boosted", "boosting",
"waxed", "upbeat", "surge")
#Adding to negative words
neg.words = c(neg_all, "earn", "shortfall", "weak", "fell",
"decreases", "decreases", "decreased", "contraction", "cutback",
"cuts", "drop", "shrinkage", "reduction", "abated", "cautious",
"caution", "damped", "waned", "undermine", "unfavorable", "soft",
"softening", "soften", "softer", "sluggish", "slowed", "slowdown",
"slower", "recession")
# Corpus exploration indicates these words should
# be removed from our corpus
bb.stopwords <- c(stnd.stopwords, "district",
"districts", "reported", "noted",
"city", "cited", "activity",
"contacts", "chicago", "dallas",
"kansas", "san", "richmond",
"francisco", "cleveland", "atlanta",
"sales", "boston", "york",
"philadelphia", "minneapolis",
"louis", "services", "year",
"levels")
# useful to eliminate words that lack
# discriminatory power. bb.tf will be used as a control
# for the creation of our term-document matrix.
bb.tf <- list(weighting = weightTf,
stopwords = bb.stopwords,
removePunctuation = TRUE,
tolower = TRUE,
minWordLength = 4,
removeNumbers = TRUE)
# Remove sparse terms from the TDM w/value of 0.95,
# representing maximal allowed sparsity
BB.95 <- removeSparseTerms(bb_tdm, .95)
??removeSparseTerms
require("plyr")
require("dplyr")
require("reshape")
require("tm")
require("SnowballC")
require("ggplot2")
require("RColorBrewer")
require("wordcloud")
# Import positive lexicons
pos <- scan(file.path
("C:/Users/Justin/Documents/R",
'positive-words.txt'), what = 'character',
comment.char = ';')
# import financial positive lexicon
pos_finance <- scan(file.path(
"C:/Users/Justin/Documents/R",
'LoughranMcDonald_pos.csv'),
what = 'character',comment.char = ';')
# combine both files into one
pos_all <- c(pos, pos_finance)
# load general negative words lexicon
neg <- scan(file.path("C:/Users/Justin/Documents/R",
"neg_words.txt"),
what = 'character', comment.char = ';')
# load financial negative words lexicon
neg_finance <- scan(file.path(
"C:/Users/Justin/Documents/R",
"LoughranMcDonald_pos.csv"),
what = 'character', comment.char = ';')
# combine negative words into single file
neg_all <- c(neg, neg_finance)
# Import Beige Book
BB <- read.csv("C:\Users\Justin\Documents\GitHub\ShameVector\Case Studies\New folder\BB_96_2013.csv.txt")
# Import positive lexicons
pos <- scan(file.path
("C:/Users/Justin/Documents/R",
'positive-words.txt'), what = 'character',
comment.char = ';')
# import financial positive lexicon
pos_finance <- scan(file.path(
"C:/Users/Justin/Documents/R",
'LoughranMcDonald_pos.csv'),
what = 'character',comment.char = ';')
# combine both files into one
pos_all <- c(pos, pos_finance)
# load general negative words lexicon
neg <- scan(file.path("C:/Users/Justin/Documents/R",
"neg_words.txt"),
what = 'character', comment.char = ';')
# load financial negative words lexicon
neg_finance <- scan(file.path(
"C:/Users/Justin/Documents/R",
"LoughranMcDonald_pos.csv"),
what = 'character', comment.char = ';')
# combine negative words into single file
neg_all <- c(neg, neg_finance)
# Import Beige Book
BB <- read.csv("C:/Users/Justin/Documents/GitHub/ShameVector/Case Studies/New folder/BB_96_2013.csv.txt")
bad <- is.na(BB)
BB[bad]
BB$text <- gsub('[[:punct:]]', ' ', BB$text)
BB$text <- gsub('[[:cntrl:]]', ' ', BB$text)
BB$text <- gsub('//d+', ' ', BB$text)
BB.text <- as.data.frame(BB$text)
BB.text$year <- BB$year
BB.text$Date <- as.Date(paste(BB$year,
BB$month,
BB$day, sep = "-"),
format = "%Y-%m-%d")
BB.text$Date <- strptime(as.character(BB.text$Date),
"%Y-%m-%d")
colnames(BB.text) <- c("text", "year", "date")
# tm_map allows transformation to a corpus
bb_corpus <- Corpus(VectorSource(BB.text))
# stem the documents
bb.text_stm <- tm_map(bb_corpus, stemDocument)
# Standard stopwords ie the SMART list are in TM
stnd.stopwords<- stopwords("SMART")
bb_tdm <- TermDocumentMatrix(bb_corpus,
control = bb.tf)
bb.frequent <- sort(rowSums(as.matrix(bb_tdm)),
decreasing = TRUE)
bb.frequent[1:30]
findFreqTerms(bb_tdm, lowfreq = 60)
findFreqTerms(bb_tdm, lowfreq = 100)
findFreqTerms(bb_tdm, lowfreq = 1000)
pos.words<- c(pos_all, "spend", "buy", "earn", "hike", "increase",
"increases", "development", "expansion", "raise", "surge", "add",
"added", "advanced", "advances", "boom", "boosted", "boosting",
"waxed", "upbeat", "surge")
bb.frequent[1:30]
bb.stopwords <- c(stnd.stopwords, "continued",
"perent", "report", "remained",
"city", "cited", "activity",
"rates", "recent", "months",
"reports", "generally", "markets",
"francisco", "cleveland", "atlanta",
"sales", "boston", "york",
"philadelphia", "minneapolis",
"louis", "services", "year",
"levels")
bb.tf <- list(weighting = weightTf,
stopwords = bb.stopwords,
removePunctuation = TRUE,
tolower = TRUE,
minWordLength = 4,
removeNumbers = TRUE)
BB.95 <- removeSparseTerms(bb_tdm, .95)
BB.rsums <- sort(rowSums(as.matrix(BB.95)),
decreasing=TRUE)
BBdf.rsums <- data.frame(word=names(BB.rsums),
freq=BB.rsums)
palette <- brewer.pal(9, "BuGn")
palette <- palette[-(1:2)]
png(filename="C:/Users/Justin/Documents/GitHub/Sentiment_Monitor/Plots/BB_cloud.png")
png(filename="C:/Users/Justin/Documents/GitHub/ShameVector/Plots/BB_cloud2.png")
bb.wordcloud <- wordcloud(BBdf.rsums$word, BBdf.rsums$freq,
random.order=FALSE, colors=palette)
dev.off()
warnings()
bb.wordcloud <- wordcloud(BBdf.rsums$word, BBdf.rsums$freq,
random.order=FALSE, colors=palette)
BB.keeps <- BB.text[,c("date", "year")]
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
require("plyr")
require("dplyr")
require("reshape")
require("tm")
require("SnowballC")
require("ggplot2")
require("RColorBrewer")
require("wordcloud")
# Import positive lexicons
pos <- scan(file.path
("C:/Users/Justin/Documents/R",
'positive-words.txt'), what = 'character',
comment.char = ';')
# import financial positive lexicon
pos_finance <- scan(file.path(
"C:/Users/Justin/Documents/R",
'LoughranMcDonald_pos.csv'),
what = 'character',comment.char = ';')
# combine both files into one
pos_all <- c(pos, pos_finance)
# load general negative words lexicon
neg <- scan(file.path("C:/Users/Justin/Documents/R",
"neg_words.txt"),
what = 'character', comment.char = ';')
# load financial negative words lexicon
neg_finance <- scan(file.path(
"C:/Users/Justin/Documents/R",
"LoughranMcDonald_pos.csv"),
what = 'character', comment.char = ';')
# combine negative words into single file
neg_all <- c(neg, neg_finance)
# Import Beige Book
BB <- read.csv("C:/Users/Justin/Documents/GitHub/ShameVector/Case Studies/New folder/BB_96_2013.csv.txt")
# Import Beige Book
BB <- read.csv("C:/Users/Justin/Documents/GitHub/ShameVector/Case Studies/TestData/BB_96_2013.csv.txt")
bad <- is.na(BB)
BB[bad]
BB$text <- gsub('[[:punct:]]', ' ', BB$text)
# gsub substitues character classes that do not
# give an output, such as feed, bckspce & tab w/space
BB$text <- gsub('[[:cntrl:]]', ' ', BB$text)
# gsub subs numerical values with digits >=1 w/' '
BB$text <- gsub('//d+', ' ', BB$text)
BB.text <- as.data.frame(BB$text)
BB.text$year <- BB$year
BB.text$Date <- as.Date(paste(BB$year,
BB$month,
BB$day, sep = "-"),
format = "%Y-%m-%d")
BB.text$Date <- strptime(as.character(BB.text$Date),
"%Y-%m-%d")
colnames(BB.text) <- c("text", "year", "date")
bb_corpus <- Corpus(VectorSource(BB.text))
bb.text_stm <- tm_map(bb_corpus, stemDocument)
stnd.stopwords<- stopwords("SMART")
bb_tdm <- TermDocumentMatrix(bb_corpus,
control = bb.tf)
bb.frequent <- sort(rowSums(as.matrix(bb_tdm)),
decreasing = TRUE)
bb.frequent[1:30]
bb.frequent[1:30]
findFreqTerms(bb_tdm, lowfreq = 1000)
pos.words<- c(pos_all, "spend", "buy", "earn", "hike", "increase",
"increases", "development", "expansion", "raise", "surge", "add",
"added", "advanced", "advances", "boom", "boosted", "boosting",
"waxed", "upbeat", "surge")
neg.words = c(neg_all, "earn", "shortfall", "weak", "fell",
"decreases", "decreases", "decreased", "contraction", "cutback",
"cuts", "drop", "shrinkage", "reduction", "abated", "cautious",
"caution", "damped", "waned", "undermine", "unfavorable", "soft",
"softening", "soften", "softer", "sluggish", "slowed", "slowdown",
"slower", "recession")
findFreqTerms(bb_tdm, lowfreq = 1000)
bb.stopwords <- c(stnd.stopwords, "ago",
"average", "business", "businesses",
"period", "cited", "change",
"rates", "recent", "months",
"reports", "generally", "markets",
"francisco", "cleveland", "atlanta",
"sales", "boston", "york",
"philadelphia", "minneapolis",
"louis", "services", "year",
"levels")
bb.tf <- list(weighting = weightTf,
stopwords = bb.stopwords,
removePunctuation = TRUE,
tolower = TRUE,
minWordLength = 4,
removeNumbers = TRUE)
BB.95 <- removeSparseTerms(bb_tdm, .95)
BB.rsums <- sort(rowSums(as.matrix(BB.95)),
decreasing=TRUE)
BBdf.rsums <- data.frame(word=names(BB.rsums),
freq=BB.rsums)
palette <- brewer.pal(9, "BuGn")
palette <- palette[-(1:2)]
png(filename="C:/Users/Justin/Documents/GitHub/ShameVector/Plots/BB_cloud2.png")
bb.wordcloud <- wordcloud(BBdf.rsums$word, BBdf.rsums$freq,
random.order=FALSE, colors=palette)
dev.off()
BB.keeps <- BB.text[,c("date", "year")]
score.sentiment = function(sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, pos.words, neg.words) {
word.list = str_split(sentence, '\\s+')
words = unlist(word.list)
pos.matches = match(words, pos.words)
neg.matches = match(words, neg.words)
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(score=scores, text=sentences)
return(scores.df)
}
BB.score <- score.sentiment(BB.text$text, pos, neg,
.progress = 'text')
BB.sentiment <- cbind(BB.keeps, BB.score)
colnames(BB.sentiment)
BB.sentiment$mean <- mean(BB.sentiment$score)
BB.sum <- BB.sentiment$score
BB.sentiment$centered <- BB.sum - BB.sentiment$mean
BB.sentiment$pos[BB.sentiment$centered>0] <- 1
BB.sentiment$neg[BB.sentiment$centered<0] <- 1
BB.sentiment$pos[is.na(BB.sentiment$pos)] <- 0
BB.sentiment$neg[is.na(BB.sentiment$neg)] <- 0
sum(BB.sentiment$pos)
sum(BB.sentiment$neg)
BB.hist <- hist(BB.sentiment$score, main="Raw Sentiment Score",
xlab="Score", ylab="Frequency")
BB.hist <- hist(BB.sentiment$centered, main="Sentiment Score Centered",
xlab="Score", ylab="Frequency")
BB.boxplot <- ggplot(BB.sentiment, aes(x = BB.sentiment$year, y = BB.sentiment$centered, group = BB.text$year))
geom_boxplot(aes(fill = BB.sentiment$year), outlier.colour = "black", outlier.shape = 16, outlier.size = 2)
BB.boxplot<- BB.boxplot + xlab("Year") + ylab("Sentiment (Centered)") + ggtitle("Economic Sentiment - BeigeBook Summ.")
BB.boxplot
searchresults_list <- sapply(searchresults, function(x) x$getText())
searchresults_corpus <- Corpus(VectorSource(searchresults_list))
searchresults_corpus <- tm_map(searchresults_corpus, tolower)
searchresults_corpus <- tm_map(searchresults_corpus, removePunctuation)
searchresults_corpus <- tm_map(searchresults_corpus, function(x) removeWords(x, stopwords()))
write.csv(searchresults.df, "C:/Users/Justin/Documents/GitHub
/Sentiment_Monitor")
searchresults <- searchTwitter("@TMobile", n=1500)
library(twitteR)
library(RCurl)
library(RColorBrewer)
library(tm)
library(wordcloud)
searchresults <- searchTwitter("@TMobile", n=1500)
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="C:/Users/Justin/Documents/GitHub/Sentiment_Monitor/cacert.pem")
download.file(url="http://curl.haxx.se/ca/cacert.pem",
destfile="C:/Users/Justin/Documents/GitHub/ShameVector/Utilities/cacert.pem")
consumerKey <- "K2mpLnVJuDXlSqvC8KzaWgP5P"
consumerSecret <- "uzBix30ghVTFEciPOg3kuJLnZ4Kp2CaSJGl0FnIFNFOPBhkCn0"
access_token <- "14313727-hBhnH9BGe6hqV6nrhUyrUZ5hUi4IQmRW8zYVJgP0C"
access_token_secret <- "ZrOdr8Dv3k4wwEaOEs52hRkYTaFRoodvyfPXxfw6i8Aqa"
requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'
twittercreds <- setup_twitter_oauth(consumerKey, consumerSecret)
save(twittercreds, file="twitter authentication data.Rdata")
searchresults <- searchTwitter("@TMobile", n=1500)
searchresults.df <- do.call(rbind,
lapply(searchresults, as.data.frame))
write.csv(searchresults.df, "C:/Users/Justin/Documents/GitHub
/ShameVector/Case Studies/TestData")
write.csv(searchresults.df, "C:/Users/Justin/Documents/GitHub
/ShameVector/Case Studies/TestData/searchresults.csv")
write.csv(searchresults.df, "C:\Users\Justin\Documents\GitHub
\ShameVector\Case Studies\TestData\searchresults.csv")
write.csv(searchresults.df, "C:/Users/Justin/Documents/GitHub
/ShameVector/Case Studies/TestData/searchresults.csv")
write.csv(searchresults.df, "C:/Users/Justin/Documents/GitHub/ShameVector/Case Studies/TestData")
write.csv(searchresults.df, "C:/Users/Justin/Documents/GitHub/ShameVector/Case Studies/TestData/searchresults.csv")
require("tm")
searchresults_list <- sapply(searchresults, function(x) x$getText())
searchresults_corpus <- Corpus(VectorSource(searchresults_list))
searchresults_corpus <- tm_map(searchresults_corpus, tolower)
searchresults_corpus <- tm_map(searchresults_corpus, removePunctuation)
searchresults_corpus <- tm_map(searchresults_corpus, function(x) removeWords(x, stopwords()))
